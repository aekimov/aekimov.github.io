<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4>This guide will show you how to use LLaMA-Factory to fine-tune language models tailored to your specific needs.</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*caJp31As5lOXaj3JNjfFGA.jpeg"><figcaption>Generated by ChatGPT</figcaption></figure> <p>Fine-tuning large language models (LLMs) has become more accessible than ever. You don’t need powerful computers or deep expertise in Machine Learning and Natural Language Processing anymore. You can customize these advanced models for your specific tasks — a capability that used to be reserved for big corporations.</p> <p>We will dive in and explore how to fine-tune models and generate responses using them.</p> <h4>What is Fine-Tuning?</h4> <p><strong>Fine-tuning</strong> is the process of taking a pre-trained language model and adapting it to perform better on a specific task or within a particular domain. Pre-trained models like GPT-4 or LLaMA have been trained on extensive datasets containing diverse text from the internet. To train those models is very demanding and difficult task to implement. While they possess a broad understanding of language, they may not excel at specialized tasks out of the box.</p> <p>Imagine you have documentation <strong>specific to your company or product</strong>, and you want to develop a chatbot that can answer users’ questions based on this information. Typically, the pre-trained model doesn’t contain any data related to your company or product. By fine-tuning the model on a smaller, task-specific dataset — such as your own documentation — you enable it to adjust its internal representations to better capture the nuances and requirements of your task. This process slightly updates the model’s weights, allowing it to generate more accurate and relevant responses tailored to your application.</p> <h4>Techniques</h4> <p>It’s important to have a basic understanding of these fine-tuning methods, even if we don’t explore all the technical details. While the underlying concepts can be complex, grasping the general ideas behind techniques like <a href="https://arxiv.org/pdf/2106.09685" rel="external nofollow noopener" target="_blank"><strong>LoRA</strong></a> and <a href="https://arxiv.org/pdf/2305.14314" rel="external nofollow noopener" target="_blank"><strong>QLoRA</strong></a> is beneficial. Although the academic papers can be challenging to read, we can simplify these techniques as follows:</p> <ul> <li> <strong>LoRA</strong> adds small, low-rank matrices — called adapters — to each layer of the pre-trained model. These adapters contain additional information that enhances the model’s capabilities for our needs without significantly altering the original weights.</li> <li> <strong>QLoRA</strong> starts by compressing a large, pre-trained model through quantization, which reduces the precision of the model’s weights (for example, from 16-bit to 4-bit). This compression drastically decreases memory usage. After quantization, the model is fine-tuned using LoRA adapters. Essentially, QLoRA combines quantization and LoRA to enable efficient fine-tuning of large models on hardware with limited resources.</li> </ul> <h4>Tools</h4> <p><a href="https://github.com/hiyouga/LLaMA-Factory" rel="external nofollow noopener" target="_blank">LLaMA-Factory</a> offers a flexible solution for customizing the fine-tuning process of over 100 LLMs and abstracts much of the underlying complexity.</p> <p>Before any installations, I recommend using virtual environments for the installations, it would make your life easier:</p> <pre>conda create -n lfenv python=3.9<br>conda activate lfenv</pre> <p>The installation process is very simple:</p> <pre>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git<br>cd LLaMA-Factory<br>pip install -e ".[torch,metrics]"</pre> <blockquote><em>If you encounter any errors, please visit the GitHub page for detailed explanations and package version requirements.</em></blockquote> <p>Also you need to install <a href="https://huggingface.co/docs/transformers/installation" rel="external nofollow noopener" target="_blank">transformers</a> library from <a href="https://huggingface.co/" rel="external nofollow noopener" target="_blank">Hugging Face</a> as well as to register an account with an <a href="https://huggingface.co/settings/tokens" rel="external nofollow noopener" target="_blank">access token</a>. It will allow you to download models. The easiest way to authenticate is to save the token on your machine. You can do that from the terminal using the command:</p> <pre>huggingface-cli login</pre> <p>Additionally, for instance, when accessing models from Meta, you are required to provide your contact information by filling out a form after clicking <strong>“<em>Expand to review and access”</em></strong>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9Y1SndR9BxfJgCHlZ3tagA.png"></figure> <blockquote> <strong><em>Important Note:</em></strong><em> Access to the model is not granted immediately, you will receive an email once the verification process is completed.</em> </blockquote> <h4>Dataset Preparation</h4> <p>Here are some important points regarding the dataset preparation process. For custom dataset preparation:</p> <blockquote>The <em>dataset_info.json</em> file contains all the available datasets. If you are using a custom dataset, ensure that you include a description of the dataset in <em>dataset_info.json</em> and specify <em>dataset: dataset_name</em> before initiating training.</blockquote> <p>For our tests, we will be using the predefined alpaca_en_demo dataset, which is already included and referenced in dataset_info.json.</p> <h4>Fine-tuning</h4> <p>Starting with a brief overview of the web interface can be helpful, as it allows you to easily see and adjust various parameters. To access it:</p> <pre># We explicitly specify what GPU we are going to use.<br>set CUDA_VISIBLE_DEVICES=0 </pre> <pre>cd LLaMA-Factory<br>python src/webui.py</pre> <p>Once you do this, the web page will be opened:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wWdsQFIBpD0g2Hy6-5AC6Q.png"><figcaption>Web UI view</figcaption></figure> <p>We won’t dive into the specifics of each parameter at this time — that’s a topic for another discussion. The key steps here involve choosing a model, a fine-tuning method, and a dataset. I selected the Llama-3–3B model, the Lora fine-tuning, and the alpaca_en_demo dataset. By default, the model will be saved in the LLaMA-Factory/save folder. For now, we can leave the other parameters as they are.</p> <p>When you press Start the training will begin. When training is finished you can access the files and inspect them.</p> <p>There is another way to configure and train without web interface. You can specify the configuration .yaml file. As an example you can reuse the one that we just generated training_args.yaml and place it in LLaMA-Factory folder. Also you can inspect predefined configs in example folder by Llama-Factory creators.</p> <p>The config that I used, I changed the folder, where it will be saved, num_train_epochs to make trainig faster just for testing purposes and some logging settings of training and validation loss. Feel free to try any parameters that you like.</p> <p>When you press Start, the training process will begin. Once the training is complete, you can access and review the generated files.</p> <p><strong>Alternatively</strong>, you can configure and train without using the web interface by specifying a .yaml configuration file. For instance, you can use the training_args.yaml file that was just generated and place it in the LLaMA-Factory folder. Additionally, you can explore the predefined configurations provided by the LLaMA-Factory creators in the example folder.</p> <p>In my new configuration, I adjusted the output folder location and changed some logging settings for training and validation loss. Feel free to experiment with any parameters you wish.</p> <pre>bf16: true<br>cutoff_len: 1024<br>dataset: alpaca_en_demo<br>dataset_dir: data<br>ddp_timeout: 180000000<br>do_train: true<br>eval_steps: 10<br>eval_strategy: steps<br>finetuning_type: lora<br>flash_attn: auto<br>gradient_accumulation_steps: 8<br>include_num_input_tokens_seen: true<br>learning_rate: 5.0e-05<br>logging_steps: 1<br>lora_alpha: 16<br>lora_dropout: 0<br>lora_rank: 8<br>lora_target: all<br>lr_scheduler_type: cosine<br>max_grad_norm: 1.0<br>max_samples: 100000<br>model_name_or_path: meta-llama/Meta-Llama-3-8B<br>num_train_epochs: 3.0<br>optim: adamw_torch<br>output_dir: saves\Llama-3-8B\lora\train<br>packing: false<br>per_device_eval_batch_size: 2<br>per_device_train_batch_size: 2<br>plot_loss: true<br>preprocessing_num_workers: 16<br>report_to: none<br>save_steps: 100<br>stage: sft<br>template: default<br>val_size: 0.1<br>warmup_steps: 0</pre> <p>To start training:</p> <pre># We explicitly specify what GPU we are going to use.<br>set CUDA_VISIBLE_DEVICES=0 <br>llamafactory-cli train training_args.yaml</pre> <h4>Response Generation</h4> <p>After obtaining our fine-tuned model, we can generate responses using it. This code demonstrates how to load a pre-trained Llama-3–8B model and its tokenizer, apply the fine-tuned weights, and generate a text response based on a provided prompt. The generated text is then decoded and printed.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/10c0e06ad7662a9cd083fb9a21108ffa/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/10c0e06ad7662a9cd083fb9a21108ffa/href</a></iframe> <p>If you want to see how the model responds without fine-tuning, you can comment out this line:</p> <pre>model = PeftModel.from_pretrained(model, ft_model_path)</pre> <p>Thanks for reading this article! I hope it gave you a helpful introduction to fine-tuning and how to generate responses. I wanted to keep it simple and let you try it out without diving too deep into technical details. Feel free to leave a comment or ask any questions you have.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=874be25002e2" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://levelup.gitconnected.com/how-to-fine-tune-llms-without-being-an-ai-expert-874be25002e2" rel="external nofollow noopener" target="_blank">How to Fine-Tune LLMs Without Being an AI Expert</a> was originally published in <a href="https://levelup.gitconnected.com" rel="external nofollow noopener" target="_blank">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>